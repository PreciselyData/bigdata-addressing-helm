## Default values for geo-addressing-spark-helm.
## This is a YAML-formatted file.
## Declare variables to be passed into your templates.
global:
  nodeSelector: { }
  ##
  ## parameter to enable or disable a specific country data to be loaded in the memory
  countries:
    - usa
    - gbr
    - aus
    - nzl
    - can
  nfs:
    ##
    ## The base path where the volume should be mounted inside the container.
    volumeMountPath: /mnt/data/geoaddressing-data
    ##
    ## The base path where the data is present for verify-geocode functionalities.
    ##
    ## [NOTE]: if your data for verify-geocode is not present in the format `verify-geocode/[country]/[timestamp]/[vintage]`, this parameter needs to be overridden.
    addressingBasePath: verify-geocode
  dataVintage:
    configMap:
      name: geo-addressing-spark-data-mnt-config

geo-addressing-spark-hook:
  enabled: true

roleName: spark-role

image:
  repository: # mandatory parameter to be provided in the command
  tag: "0.1.0"
  pullPolicy: "Always"

imagePullSecrets: [ ]
nameOverride: ""
fullnameOverride: ""

serviceAccount:
  # Specifies whether a service account should be created
  create: false
  # Automatically mount a ServiceAccount's API credentials?
  automount: true
  # Annotations to add to the service account
  annotations: { }
  # The name of the service account to use.
  # If not set and create is true, a name is generated using the fullname template
  name: "spark"

securityContext: { }
  # capabilities:
  #   drop:
#   - ALL
# readOnlyRootFilesystem: true
# runAsNonRoot: true
# runAsUser: 1000

resources: { }
  # We usually recommend not to specify default resources and to leave this as a conscious
  # choice for the user. This also increases chances charts run on environments with little
  # resources, such as Minikube. If you do want to specify resources, uncomment the following
  # lines, adjust them as necessary, and remove the curly braces after 'resources:'.
  # limits:
  #   cpu: 100m
#   memory: 128Mi
# requests:
#   cpu: 100m
#   memory: 128Mi

livenessProbe:
  httpGet:
    path: /
    port: http
readinessProbe:
  httpGet:
    path: /
    port: http

# Additional volumes on the output Deployment definition.
volumes: [ ]
# - name: foo
#   secret:
#     secretName: mysecret
#     optional: false

# Additional volumeMounts on the output Deployment definition.
volumeMounts: [ ]
# - name: foo
#   mountPath: "/etc/foo"
#   readOnly: true

tolerations: [ ]

spark:
  ## Spark Version is linked with the docker image that is being provided.
  ## Please change or download the required docker image for the specific spark version.
  version: "3.5.0"
  app_name: "localApp"
  log_level: "warn"
  dynamic_allocation: true
  initial_executors: 1
  min_executors: 1
  max_executors: 10
  driver:
    cores: 1
    memory: "2g"
  executor:
    cores: 7
    memory: "16g"
  conf:
    "spark.sql.streaming.schemaInference": "true"
    "spark.sql.files.maxPartitionBytes": "64430"

secrets:
  ACCESS_KEY: # Need to provide this explicitly
  SECRET_KEY: # Need to provide this explicitly

env:
  ENV: dev
  IS_STREAMING: true #Keep it to true. Setting it to false will start the app in batch mode but may lead to unexpected errors.
  OPERATION: "geocode"
  IN_SOURCE: "s3"
  OUT_SOURCE: "s3"
  kafka:
    INPUT_TOPIC:
    INPUT_BOOTSTRAP_SERVER:
    INPUT_SCHEMA:
    OUTPUT_TOPIC:
    OUTPUT_BOOTSTRAP_SERVER:
  file:
    INPUT_FILE_TYPE: "csv"
    OUTPUT_FILE_TYPE: "csv"
    INPUT_PATH:
    OUTPUT_PATH:
    USE_HIERARCHY: "true"
  RETAIN_COLUMNS: true
  ADDITIONAL_OPTIONS_READ:
  ADDITIONAL_OPTIONS_WRITE:
  ERROR_FIELD: "error"
  JSON_RESPONSE:
  INPUT_FIELDS:
  OUTPUT_FIELDS:
  REPARTITION_NUM:
  COALESCE_NUM:
